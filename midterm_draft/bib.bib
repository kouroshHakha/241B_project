
@article{wang_error_2015,
	title = {Error {Adaptive} {Classifier} {Boosting} ({EACB}): {Leveraging} {Data}-{Driven} {Training} {Towards} {Hardware} {Resilience} for {Signal} {Inference}},
	volume = {62},
	issn = {1549-8328, 1558-0806},
	shorttitle = {Error {Adaptive} {Classifier} {Boosting} ({EACB})},
	url = {http://ieeexplore.ieee.org/document/7070874/},
	doi = {10.1109/TCSI.2015.2395591},
	abstract = {The continued scaling of CMOS technologies and consideration of post-CMOS technologies has elevated hardware reliability to a ﬁrst-class challenge, particularly in energy- and resource-constrained embedded sensor applications. In such applications, there is an increasing emphasis on inference functions. Machine-learning algorithms play an important role by enabling the construction of data-driven models for inference over data that is too complex to model analytically. This paper explores how datadriven training can be exploited to also overcome computational errors due to hardware faults within an inference stage. FPGA emulation with randomized fault injections shows that the proposed architecture restores system performance to the level of a fault free system, with 1\% of the hardware requiring explicit fault protection, and with digital faults affecting 2\% of the circuit nodes in the rest of the hardware. To train an error-aware inference model, a training algorithm is presented whose hardware (memory) and energy requirements are reduced by 65 and 10 compared to previously reported algorithms (AdaBoost and FilterBoost respectively), thereby enabling model construction entirely on the device.},
	number = {4},
	urldate = {2018-03-15},
	journal = {IEEE Transactions on Circuits and Systems I: Regular Papers},
	author = {Wang, Zhuo and Schapire, Robert E. and Verma, Naveen},
	month = apr,
	year = {2015},
	pages = {1136--1145},
	file = {Wang et al. - 2015 - Error Adaptive Classifier Boosting (EACB) Leverag.pdf:/Users/kourosh_hakhamaneshi/Zotero/storage/DPTSA63F/Wang et al. - 2015 - Error Adaptive Classifier Boosting (EACB) Leverag.pdf:application/pdf}
}

@article{zhang_-memory_2017,
	title = {In-{Memory} {Computation} of a {Machine}-{Learning} {Classifier} in a {Standard} 6T {SRAM} {Array}},
	volume = {52},
	issn = {0018-9200, 1558-173X},
	url = {http://ieeexplore.ieee.org/document/7875410/},
	doi = {10.1109/JSSC.2016.2642198},
	abstract = {This paper presents a machine-learning classiﬁer where computations are performed in a standard 6T SRAM array, which stores the machine-learning model. Peripheral circuits implement mixed-signal weak classiﬁers via columns of the SRAM, and a training algorithm enables a strong classiﬁer through boosting and also overcomes circuit nonidealities, by combining multiple columns. A prototype 128 × 128 SRAM array, implemented in a 130-nm CMOS process, demonstrates ten-way classiﬁcation of MNIST images (using image-pixel features downsampled from 28 × 28 = 784 to 9 × 9 = 81, which yields a baseline accuracy of 90\%). In SRAM mode (bit-cell read/write), the prototype operates up to 300 MHz, and in classify mode, it operates at 50 MHz, generating a classiﬁcation every cycle. With accuracy equivalent to a discrete SRAM/digital-MAC system, the system achieves ten-way classiﬁcation at an energy of 630 pJ per decision, 113× lower than a discrete system with standard training algorithm and 13× lower than a discrete system with the proposed training algorithm.},
	number = {4},
	urldate = {2018-03-15},
	journal = {IEEE Journal of Solid-State Circuits},
	author = {Zhang, Jintao and Wang, Zhuo and Verma, Naveen},
	month = apr,
	year = {2017},
	pages = {915--924},
	file = {Zhang et al. - 2017 - In-Memory Computation of a Machine-Learning Classi.pdf:/Users/kourosh_hakhamaneshi/Zotero/storage/WAZU5Z5E/Zhang et al. - 2017 - In-Memory Computation of a Machine-Learning Classi.pdf:application/pdf}
}